+ **自监督学习**（Self-Supervised Learning，SSL）是一种无标注的学习方式，由Yann LeCun提出。![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605101015.png)
+ 假设我们有未标注的文章数据，则可将一篇文章 x，将 x 分为两部分：**模型的输入 x′ 和模型的标签 x′′，将 x′ 输入模型并让它输出 yˆ，想让 yˆ 尽可能地接近它的标签 x′′（学习目标）**，这就是自监督学习.
+ 由于自监督学习不使用标注的数据，因此自监督学习可以看作是一种无监督学习方法. 为什么不直接称其为无监督学习？因为无监督学习是一个比较大的家族，里面有很多不同的方法，自监督学习只是其中之一. 为了使定义更清晰，称其为自监督学习.
+ 自监督学习的模型大多都是以芝麻街的角色命名，来让其名称缩写“凑”成电视节目《芝麻街》中的角色，以下是几个例子：
	+ **ELMo**：来自语言模型的嵌入（Embeddings from Language Modeling），名称来自《芝麻街》的红色小怪兽 Elmo，ELMo 是最早的自监督学习的模型；
	+ **BERT**：来自 Transformers 的双向编码器表示（Bidirectional Encoder Representation from Transformers），名称来自《芝麻街》的另一个角色 Bert；
	+ BERT 提出后，马上就出现了两个不同的模型，都叫**ERNIE**，一个模型是知识增强的语义表示模型（Enhanced Representation through Knowledge Integration），另一个模型是具有信息实体的增强语言表示（Enhanced Language Representation withInformative Entities），名称来自 Bert 最好的朋友 Ernie；
	+ **Big Bird**：较长序列的 Transformer（Transformers for Longer Sequences，Big Bird），名称来自《芝麻街》的黄色大鸟 Big Bird.
+ 自监督模型的参数都很大.Megatron 的参数量是生成式预训练-2（Gen-erative Pre Training-2，GPT-2）的 8 倍左右.GPT-3 的参数量是 Turing NLG 的 10 倍. 目前最大的模型是谷歌的 Switch Transformer，其参数量比 GTP-3 大了 10 倍.

## 10.1 来自Transformers的双向编码器表示（BERT）
+ BERT 是一个 Transformer 的编码器，BERT 的架构与 Transformer 的编码器完全相同，里面有很多自注意力和残差连接、归一化等等.**BERT 可以输入一行向量，输出另一行向量. 输出的长度与输入的长度相同.**![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605102105.png)
+ BERT 一般用在自然语言处理中，用在文本场景中，所以一般它的输入是一个文本序列，也是一个数据序列. 不仅文本是一种序列，语音也可以看作是一种序列，甚至图像也可以看作是一堆向量. 因此 BERT 不仅可以用在自然语言处理中，也用在文本中，它还可以用于语音和视频. 因为 BERT 最早是用在文本中，所以这里都以文本为例（语音或图像也都是一样的）BERT 的输入是一段文字. 接下来需要随机掩码一些输入文字，被掩码的部分是随机决定的.
+ 例如，输入 100 个词元. 什么是词元？**词元是处理一段文本时的基本单位，词元的单位大小由我们自己决定. 在中文文本中，通常将一个汉字当成一个词元**. 当输入一个句子时，里面有一些单词会被随机掩码. 哪些部分需要掩码？它是随机决定的.实现掩码的两种方法：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605103654.png)
	+ **添加一个名为“MASK”的特殊词元：** 第一种方法是用特殊符号替换句子中的单词，使用“MASK”词元来表示特殊符号，可以将其看成一个新的汉字，它不在字典里，它的意思是掩码原文. **掩码的目的是对向量中某些值进行掩盖，避免无关位置的数值对运算造成影响**
	+ **用另一个词替换某个词**：另一种方法是用另一个字随机替换一个字. 本来是“度”字，可以随机选择另一个汉字来替换它，比如改成“一”/“天”/“大”/“小”，只是用随机选择的某个字替换它
+ 这两种方法都可以使用，使用哪种方法也是随机确定的. 所以在 BERT 训练的时候，**应该给BERT 输入一个句子，首先随机决定要掩码哪些汉字，之后，再决定如何进行掩码**. 
+ **BERT预测过程**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605221126.png)
+ 掩码后，向 BERT 输入了一个序列，BERT 的相应输出就是另一个序列.接下来，查看输入序列中掩码部分的对应输出，仍然在掩码部分输入汉字，它可能是“MASK”词元或随机单词，它仍然输出一个向量，对这个向量使用**线性变换**（线性变换是指输入向量会乘以一个矩阵）. 然后做 **softmax** 并输出一个分布. **输出是一个很长的向量，包含要处理的每个汉字. 每个字对应一个分数，它是通过 softmax 函数生成的分布**.

+ **BERT训练过程**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605221317.png)
+ 我们知道被掩码字符是哪个字符，而 BERT 不知道. 因为把句子交给 BERT 时，该字符被掩码了，所以 BERT 不知道该字符，但我们知道掩码字符“深度”一词中的“度”. 因此，**训练的目标是输出一个尽可能接近真实答案的字符**，即“度”字符. **独热编码可以用来表示字符，并最小化输出和独热向量之间的交叉熵损失**. 这个问题可以看成一个**分类问题**，只是类的数量和汉字的数量一样多. 如果汉字的数量大约在 4000左右，该问题就是一个 4000 类的分类问题.BERT 要做的就是成功预测掩码的地方属于的类别，在这个例子里，就是“度”类别. 在训练过程中，在 BERT 之后添加一个线性模型并将它们一起训练. 所以，BERT 内部是一个 Transformer 的编码器，它有一堆参数. 线性模型是一个矩阵，它也有一些参数，尽管与 BERT 相比，其数量要少得多. 我们需要联合训练 BERT 和线性模型并尝试预测被掩码的字.

+ **下一句预测（next sentenceprediction）**：我们可以通过在互联网上使用爬虫来获得的大量句子来构建数据库，然后从数据库中拿出两个句子. 如图所示：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605221853.png)
+ 这两个句子中间加入了一个特殊的**词元 [SEP] 来代表它们之间的分隔**. 这样，BERT 就可以知道这两个句子是不同的句子，因为这两个句子之间有一个分隔符号. 我们还将会在**整个序列的最前面加入一个特殊词元分类符号 [CLS]**.现在给定一个很长的序列，其中包括两个句子，中间有个 [SEP] 词元，前面有一个 [CLS]词元. 如果将这个很长的序列输入到 BERT，它应该输出一个序列，按理说，输入是一个序列，输出应该是另外一个序列，这是编码器可以做的事情. 而 BERT 就是一个 Transformer 的编码器，所以 BERT 可以做这件事. 
+ 我们只取与 [CLS] 对应的输出，忽略其他输出，并将 [CLS]**的输出乘以线性变换**. 现在它做一个二元分类问题，它有两个可能的输出：是或否. 这种方法是下一句预测，即**需要预测第二句是否是第一句的后一句（这两个句子是不是相接的）**. 如果第二句确实是后续句子（这两个句子是相接的），就要训练 BERT 输出“是”. 当第二句不是后一句时（这两个句子不是相接的），BERT 需要输出“否”作为预测.
+ 但后来的研究发现，**下一句预测对 BERT 将要完成的任务并没有真正的帮助**. 有一篇题为“Robustly Optimized BERT Approach（RoBERTa）”的论文明确指出使用下一句预测方法几乎没有帮助，之后，这个想法以某种方式成为主流. 紧接着，另一篇论文说下一句预测没用，后来又有很多论文开始说它也没用，比如 SCAN-BERT 和 XLNet. 下一句预测没用的可能原因之一是**下一句预测这个任务太简单了，这是一项容易的任务，预测两个句子是否相接并不是一项特别困难的任务**. 此任务的通常方法是首先随机选择一个句子，然后从数据库中随机选择将要连接到前一个句子的句子. 通常，随机选择一个句子时，它很可能与之前的句子有很大不同. 对于 BERT 来说，预测两个句子是否相接并不难. 因此，在训练 BERT 完成下一句预测任务时，没有学到太多有用的东西.

+ 还有一种类似于下一句预测的方法——**句序预测（Sentence Order Prediction，SOP）**，其在文献上似乎更有用. 这种方法的主要思想是**最初选择的两个句子本来就是连接在一起，可能有两种可能**：
	+ 要么句子 1 连接在句子 2 后面
	+ 要么句子 2 连接在句子 1 后面
+ 有两种可能性，BERT 要回答是哪一种可能性. 或许是**因为这个任务难度更大，所以句序预测似乎更有效**. 它被用于名为 **ALBERT** 的模型中，该模型是 BERT 的进阶版本.

### 10.1.1 BERT的使用方式
+ 训练时，让BERT学习两个任务：
	+ 把一些字符掩盖起来，让它做填空题，补充掩码的字符.
	+ 预测两个句子是否有顺序关系（两个句子是否应该接在一起），这个任务没什么用.
+ 通过这两个任务，BERT 学会了如何填空，但仅此而已。下面这些任务是真正使用 BERT 的任务，可称为**下游任务（downstream task）**.下游任务就是我们实际关心的任务. BERT学习完成这些任务时，仍然需要一些标注的数据（就像胚胎干细胞可以分化出各种不同的细胞，具有无限的潜力）![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605222557.png)
+ **将 BERT 分化并用于各种任务称为微调（fine-tuning）**。**与微调相反，在微调之前产生此 BERT 的过程称为预训练**. 所以**产生 BERT 的过程就是自监督学习，也可以将其称为预训练**.
+ 在谈如何对 BERT 进行微调之前，先看看它的能力. 要测试自监督学习模型的能力，通常会在多个任务上进行测试，得到各自的正确率，再取平均值。

+ 对模型进行测试的不同任务的这种集合，可以将其称为任务集. 任务集中最著名的标杆（基准测试）称为**通用语言理解评估（General Language UnderstandingEvaluation，GLUE）**.GLUE 里面一共有 **9 个任务**：
	+ 语言可接受性语料库（the Corpus of Linguistic Acceptabil-ity，CoLA）
	+ 斯坦福情感树库（the Stanford Sentiment Treebank，SST-2）
	+ 微软研究院释义语料库（the Microsoft Research Paraphrase Corpus，MRPC）
	+ 语义文本相似性基准测试（theSemantic Textual Similarity Benchmark，STSB）
	+ Quora 问题对（the Quora Question Pairs，QQP）
	+ 多类型自然语言推理数据库（the Multi-genre Natural Language Inference corpus，MNLI）
	+ 问答自然语言推断（Qusetion-answering NLI，QNLI）
	+ 识别文本蕴含数据集（theRecognizing Textual Entailment datasets，RTE）
	+ Winograd 自然语言推断（Winograd NLI，WNLI）
+ 我们要把机器与人类进行比较：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605223216.png)

+ 下面介绍四个使用BERT的情况：
#### 10.1.1.1 情感分析
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605224014.png)
+ 输入一个序列并输出一个类别。例如情感分析：**给机器一个句子，并告诉它判断句子是正面的还是负面的**. 对于 BERT，它是如何解决情感分析的问题的？如图所示，只要给它一个句子，把 [CLS] 词元放在这个句子前面.[CLS]、w1、w2、w3 4 个输入对应于 4 个输出. 接着，对 [CLS] 对应的向量应用线性变换，将其乘上一个矩阵. 这里省略了softmax，通过 softmax 来确定输出类别是正面的或负面的等等. 但是，**必须要有下游任务的标注数据**。(BERT 没有办法从头开始解决情感分析问题，其仍然需要一些标注数据，需要提供很多句子以及它们的正面或负面标签来训练 BERT 模型) 
+ 在训练过程中，BERT 与这种线性变换放在一起，称为完整的情感分析模型. 在训练时，**线性变换和 BERT 模型都利用梯度下降来更新参数. 线性变换的参数是随机初始化的，而 BERT 初始的参数是从学习了做填空题的 BERT来的**. 在训练模型时，会**随机初始化参数，接着利用梯度下降来更新这些参数，最小化损失.但在 BERT 中不必随机初始化所有参数，随机初始化的参数只是线性变换的参数**.
+ BERT的骨干（backbone）是一个巨大的 Transformer 编码器，**直接拿已经学会填空的 BERT 的参数当作初始化的参数**，它比随机初始化参数的网络表现更好。
+ 如图所示，横轴是训练的回合，纵轴是训练损失. 随着训练的进行，损失会越来越低.  **“微调”意味着模型有预训练**. 网络的BERT 部分（网络的编码器），该部分的参数是由学会做填空的 BERT 的参数来做初始化的.从头开始训练（scratch）意味着整个模型，包括 BERT 和编码器部分都是随机初始化的. 虚线是从头开始训练，如果是从头开始训练，在训练网络时，与使用会做填空的 BERT 进行初始化的模型相比，损失下降的速度相对较慢. 随机初始化参数的网络损失仍然高于使用填空题来初始化 BERT 的网络. 所以这就是 BERT 的好处.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605224244.png)
+ Q：BERT的训练方法是半监督还是无监督？
	+ 在学习填空时，BERT 是无监督的. 但使用 BERT 执行下游任务时，下游任务需要有标注的数据. 自监督学习会使用大量未标注的数据，但下游任务有少量有标注的数据，所以合起来是半监督. 半监督是指我们有大量的未标注的数据和少量标注数据，这种情况称为半监督. 所以使用 BERT 的整个过程就是使用预训练和微调，它可以被视为一种**半监督**的方法.

#### 10.1.1.2 词性标注
+ 第二种情况是输入一个序列，然后输出另一个序列，但**输入和输出的长度是一样的**. 例如，**词性标注（Part-Of-Speech tagging，POS tagging）**。**词性标注是指给定机器一个句子，其可以知道该句子中每个单词的词性. 即使这个词是相同的，它也可能有不同的词性**。
+ 向 BERT 输入一个句子之后，对于这句话中的每个词元，如果是中文，就是每一个字，每个字都有一个对应的向量.然后把这些向量依次通过线性变换和 softmax 层. 最后，网络预测给定单词所属的类别. 例如，词性. 
+ 如果任务不同，对应的类别也会不同. 接下来和情况 1 完全一样. 换句话说，要有一些带标签的数据. 这**仍然是一个典型的分类问题**. 唯一不同的是 BERT 部分，**网络的编码器部分，其参数不是随机初始化的，它已经在预训练过程中找到了一组比较好的初始化的参数.**![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605224625.png)

#### 10.1.1.3 自然语言推理
+ 在情况 3 中，**模型输入两个句子并输出一个类别**. 这里的例子都是自然语言处理的例子，但可以将这些例子更改为其他任务，例如语音任务或计算机视觉的任务. 语音、文本和图像可以表示为一行向量，因此该技术不仅限于处理文本，还可以用于其他任务. 
+ 什么样的任务需要这样的输入和输出？最常见的一种是自然语言推理（Natural Language Inference，NLI）. **给机器两个输入语句：前提（premise）和假设（hypothesis）**. 机器所做的是**判断是否可以从前提中推断出假设，即前提与假设矛盾或者不矛盾？** 
+ 如图所示，前提是“一个骑马的人跳过一架坏掉的飞机（Aperson on a horse jumps over a broken down airplane）”，这是一个基准语料库中的例子. 而假设是这个人在一家餐馆里（Aperson is at a diner），这是一个**矛盾**. 机器要做的就是将两个句子作为输入，输出这两个句子之间的关系. 这种任务很常见，例如，立场分析. 给定一篇文章，其下面有留言，要判断留言是赞成这篇文章的立场还是反对这篇文章的立场. 只需将文章和留言一起放入模型中，模型要预测的是赞成还是反对.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605224943.png)
+ BERT 如何解决这个问题？如图所示，给定两个句子，这两个句子**之间有一个特殊的分隔词元 [SEP]**，并**把 [CLS] 词元放在最前面的位置**. 这个序列是 BERT 的输入，然后BERT 将输出另一个长度与输出长度相同的序列. 但**只将 [CLS] 词元作为线性变换的输入，然后决定输入这两个句子，输出应该是什么类别**. 对于 NLI，要输出这两个句子是否矛盾，**仍然需要标注数据**来训练这个模型.**BERT 的这部分不再是随机初始化的，它使用预训练的权重进行初始化**.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605225013.png)

#### 10.1.1.4 基于提取的问答
+ 第四个情况是问答系统，给机器读一篇文章，问它一个问题，它就会回答一个答案. 但这里的问题和答案有些限制，假设答案必须出现在文章里面，答案一定是文章中的一个片段，这是**基于提取的问答（extraction-based question answering）**. 在此任务中，**输入序列包含一篇文章和一个问题. 文章和问题都是一个序列**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605225316.png)
+ 如图所示，将 D 和 Q 放入问答模型中，希望它输出两个正整数 s 和 e. 根据 s 和e 可以直接从文章中截出一段就是答案，文章中第 s 个单词到第 e 个单词的片段就是正确答案. 这是当今使用的一种非常标准的方法.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605225347.png)

+ 当然，我们不会从头开始训练问答模型，而会使用 **BERT 预训练模型**. 如何用预先训练好的 BERT 解决这种问答问题呢？如图所示，给 BERT 看一个问题、一篇文章. **问题和文章之间有一个特殊标记 [SEP]. 然后在开头放了一个 [CLS] 词元**，这与自然语言推理的情况相同. 在自然语言推理中，一个句子是前提，一个句子是结论，而在这里，一个是文章，一个是问题. **在此任务中，唯一需要从头开始训练的只有两个向量（“从头开始训练”是指随机初始化）**，我们使用橙色向量和蓝色向量来表示它们，**这两个向量的长度与 BERT 的输出是相同的.**
+ 假设 BERT 的输出是 768 维向量，这两个向量也就是 768 维向量. 如何使用这两个向量呢？如图 (a) 所示，**首先计算橙色向量和文档对应的输出向量的内积（inner product）**.由于有 3 个词元代表文章，因此它将输出 3 个向量. 计算这 3 个向量与橙色向量的内积可以得到 3 个值. 然后将它们传递给 softmax 函数，将得到另外 3 个值. **这种内积与注意力非常相似**. 如果把橙色部分可以视为查询，把黄色部分视为键，这就是一种注意力，应该尝试找到得分最高的位置. 橙色向量和 d2 的内积最大，则 s 应等于 2，输出的起始位置应为 2.
+ 如图 (b) 所示，蓝色部分代表答案结束的地方. 计算蓝色向量和文章对应的黄色向量的内积，接着对内积使用 softmax 函数. 最后，找到最大值. 如果第 3 个值最大，e 应为 3.正确答案是 d2 和 d3，所以模型要做的实际上是预测正确答案的起始位置，因为答案一定在文章里. 如果文章中没有答案，就不能使用这个技巧. 这里假设答案一定在文章中，必须在文章中找到答案的起始位置和结束位置. 这是问答模型需要做的. 当然，我们需要一些训练数据才能训练这个模型. **请注意，蓝色和橙色向量是随机初始化的，而 BERT 是由其预训练的权重初始化的.**![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240605225526.png)

+ **BERT输入长度限制**：理论上没有. 实际上有，理论上，因为 BERT 模型是一个 Transformer 编码器，所以它可以输入很长的序列. 只要我们有能力做自注意力. 但是自注意力的运算量很高，所以在实践中，BERT 无法真正输入太长的序列，最多可以输入 512 长度的序列. 如果输入一个 512 长度的序列，中间的自注意力即将生成 512 乘以 512 大小的注意力度量（metric），计算量会非常大，所以实际上 BERT 的长度不是无限长的.

### 10.1.2 BERT有用的原因
+ 当输入一串文字时，每个文字都有一个对应的向量，这个向量称为**嵌入**。这个向量代表了输入字的意思. 例如，模型输入“深度学习”，输出 4 个向量. 这 4 个向量代表“深”、“度”、“学”和“习”的意思![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606152041.png)
+ **把这些字对应的向量一起画出来并计算它们之间的距离，意思越相似的字，它们的向量就越接近**. 例如，“果”和“草”都是植物，它们的向量就比较接近. “鸟”和“鱼”是动物，所以它们可能更接近. “电”既不是动物也不是植物，所以比较远. 中文会有歧义（一字多义），很多语言也都有歧义.BERT 可以考虑上下文，所以同一个字，例如“果”这个字，它的上下文不同，它的向量是不会一样的. 所以吃苹果的果和苹果手机的果都是“果”，但根据上下文，它们的意思不同，所以他们对应的向量就会不一样. 吃苹果的“果”可能更接近“草”，苹果手机的“果”可能更接近“电”.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606152238.png)
+ 我们通过计算向量之间的余弦相似度来衡量嵌入的关联度：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606152450.png)![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606152522.png)
+ BERT从上下文推断出单词的意思. 训练 BERT 时，给它 w1、w2、w3 和 w4，掩码 w2，它会从上下文中提取信息来预测 w2. 所以这个向量就是它的上下文信息的精华，可以用来预测 w2 是什么.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606152723.png)
+ BERT 还可以**根据不同的上下文从相同的词汇中产生不同的嵌入**，因为它是词嵌入的高级版本，考虑了上下文.BERT 抽取的这些向量或嵌入也称为**语境化的词嵌入（contextualizedword embedding）**. 训练在文字上的 BERT 也可以用来对蛋白质、DNA 和音乐进行分类.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606153143.png)

### 10.1.3 BERT的变种——多语言 BERT（multi-lingual BERT）
+ 不管使用中文还是英文，对于意思相同的词，它们的嵌入都会很近。尽管未受过中英互译训练，也可以完成得很好![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606153336.png)

## 10.2 生成式预训练(GPT)
+ 在自监督学习中，除了 BERT 系列的模型，还有一个非常有名的模型—–GPT 系列的模型.BERT 做的是填空题，而 **GPT 就是改一下在自监督学习的时候要模型做的任务**.GPT要做的任务是**预测接下来会出现的词元**. 
+ 如图所示，例如，假设训练数据里面，有一个句子是“深度学习”. 给 GPT 输入词元 <BOS>（beginning of sentence），GPT 会输出一个嵌入（embedding）. 接下来用这个嵌入去预测下一个应该出现的词元. 在这个句子里面，根据这笔训练数据，下一个应该出现的词元是“深”. 训练模型时，根据第一个词元，根据 <BOS>的嵌入，它要输出词元“深”. ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606153910.png)
+ 接下来讲下这个部分的具体操作，对一个嵌入 h 进行一个**线性变换**，再进行一个 **softmax操作**可以得到一个分布. 跟一般做分类的问题是一样的，输出的分布跟正确答案的交叉熵越小越好. 也就是要去预测下一个出现的词元. 接下来要做的事情就是以此类推了，给 GPT 输入 <BOS> 跟“深”，它产生嵌入. 接下来它会预测下一个出现的词元，告诉它说下一个应该出现的词元是“度”. 再反复继续下去，给它 <BOS>、“深”和“度”，然后预测下一个应该出现的词元，它应该要预测“学”. 给 GPT 输入 <BOS>、“深”“度”和“学”，接下来下一个应该出现的词元是“习”，因此它应该要预测出下一个应该出现的词元是“习”.
+ 实际上不会只用一个句子训练 GPT，而是用成千上万个句子来训练模型，GPT 用了很多数据训练了一个非常大的模型.GPT 模型建立在 Transformer 的解码器的基础上，不过其会做 mask 的注意力，给定 <BOS> 预测“深”的时候，不会看到接下来出现的词汇. 给 GPT“深”要预测“度”的时候，其不会看到接下来要输入的词汇，以此类推. 因为 GPT 可以预测下一个词元，所以它有生成的能力，可以让它不断地预测下一个词元产生完整的文章，大家提到 GPT 的时候，往往会想到独角兽. 因为 GPT 系列最知名的一个例子，就是用 GPT 写了一篇跟独角兽有关的假新闻，假新闻里面说在安第斯山脉发现了独角兽。

+ GPT 系列可以把一句话补完，如何把一句话补完用在下游的任务上呢？例如，怎么把GPT 用在问答或者是其他的跟自然语言处理有关的任务上呢？GPT 可以跟 BERT 用一样的做法，BERT 是把 Transformer 编码器后面接一个简单的线性的分类器，也可以把 GPT 拿出来接一个简单的分类器，这也是会有效的，但是在 GPT 的论文没有这样做.GPT 模型太大了，大到连微调可能都有困难.
+ 在用 BERT 的时候，要把 BERT 模型后面接一个线性分类器，然后 BERT 也是要训练的模型的一部分，所以它的参数也是要调的，只需要微调它就好了，但是微调还是要花时间的. 而 GPT 实在是太过巨大，巨大到要微调它，要训练一个轮次可能都有困难. 因此 GPT系列有一个和人类更接近的使用方式。

+ 我们希望GPT也能举一反三，可以进行**小样本学习**（即在小样本上的快速学习能力. 每个类只有 k 个标注样本，k 非常小.如果 k =1，称为**单样本学习**（one-shot learning）；如果 k =0，称为**零样本学习**（zero-shot learning））
+ GPT 中的小样本学习不是一般的学习，这里面**完全没有梯度下降**，训练的时候就是要跑梯度下降，而 GPT 中完全没有梯度下降，完全没有要去调 GPT 模型参数的意思. 这种训练称为**语境学习（in-context learning）**，代表它不是一种一般的学习，它连梯度下降都没有做.![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606154638.png)

+ 其实GPT也可以应用于CV和语音上![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240606154520.png)

